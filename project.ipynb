{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e9fe20",
   "metadata": {},
   "source": [
    "# Identifying Out-of-Tune Instruments in Multi-Instrument Mixes using VGGish Transfer Learning\n",
    "\n",
    "**Team Members**: [Insert names]  \n",
    "**Course**: CS8321 - Advanced Machine Learning and Neural Networks  \n",
    "**University**: Southern Methodist University, Dallas  \n",
    "**Semester**: Spring 2025\n",
    "\n",
    "This project investigates whether a machine learning model can identify which instrument in a polyphonic audio mixture is out of tune. Using synthetic data derived from the NSynth dataset and pre-trained VGGish audio embeddings, we aim to build a robust classifier capable of detecting tuning irregularities in complex musical environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29e087",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6c9bf8c",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "1. [Motivation & Research Questions](#motivation)\n",
    "2. [Related Work](#related-work)\n",
    "3. [Problem Statement & Hypothesis](#problem)\n",
    "4. [Dataset Description & Preprocessing](#dataset)\n",
    "5. [Transfer Learning: VGGish Embeddings](#transfer)\n",
    "6. [Modeling](#modeling)\n",
    "7. [Methodology](#methodology)\n",
    "8. [Preliminary Analysis & Results](#results)\n",
    "9. [Evaluation Metrics](#evaluation)\n",
    "10. [Ethical Considerations](#ethics)\n",
    "11. [Future Work](#future)\n",
    "12. [References](#references)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e0fbbf",
   "metadata": {},
   "source": [
    "## Motivation & Research Questions <a name=\"motivation\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "762d1067",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f1fdaa0",
   "metadata": {},
   "source": [
    "## Related Work <a name=\"related-work\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8008f26a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efa90f0b",
   "metadata": {},
   "source": [
    "## Problem Statement & Hypothesis <a name=\"problem\"></a>\n",
    "\n",
    "**Problem Statement**:  \n",
    "Detect and localize which instrument in a multi-instrument audio mix is out of tune.\n",
    "\n",
    "**Hypothesis**:  \n",
    "We hypothesize that VGGish embeddings retain enough frequency-shift sensitivity to enable binary (in-tune vs. out-of-tune) and multi-class (instrument identification) classification, even when audio sources are blended.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc35b4e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70976f8e",
   "metadata": {},
   "source": [
    "## Dataset Description & Preprocessing <a name=\"dataset\"></a>\n",
    "\n",
    "We used the **NSynth-train** dataset from Activeloop’s DeepLake hub and applied the following steps:\n",
    "\n",
    "- Randomly selected 3 instruments per mix (total 1000 samples).\n",
    "- Applied pitch shift of ±1–2 semitones to one instrument per sample.\n",
    "- Mixed audio clips to form polyphonic audio.\n",
    "- Normalized and exported as `.wav` files.\n",
    "- Saved instrument labels and pitch shift metadata in `labels.csv`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8602222",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "076acd1a",
   "metadata": {},
   "source": [
    "## Transfer Learning: VGGish Embeddings <a name=\"transfer\"></a>\n",
    "\n",
    "We leverage **VGGish**, a pretrained audio feature extractor developed by Google, based on the VGG architecture. It converts audio into 128-dimensional embeddings suitable for downstream tasks.\n",
    "\n",
    "### Why VGGish?\n",
    "- Trained on large-scale YouTube data\n",
    "- Robust across audio types (speech, music, environmental sounds)\n",
    "- Eliminates need for custom feature engineering\n",
    "\n",
    "### Preprocessing for VGGish:\n",
    "- Convert `.wav` to mono, 16kHz\n",
    "- Slice or pad into 0.96s frames\n",
    "- Extract VGGish embeddings per file for classifier input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4028678a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f278965",
   "metadata": {},
   "source": [
    "## Modeling <a name=\"modeling\"></a>\n",
    "\n",
    "### Baseline Model:\n",
    "- Multi-Layer Perceptron (MLP) classifier on mean VGGish embeddings\n",
    "- Output: Multi-class classification (which instrument is out-of-tune)\n",
    "\n",
    "### Advanced Options (Optional):\n",
    "- Random Forest or Gradient Boosted Trees\n",
    "- Add attention layer on top of VGGish sequence embeddings\n",
    "- Use CNN over time-distributed embeddings\n",
    "- Explore transformer-based classifiers for temporal audio patterns\n",
    "\n",
    "### Input–Output Format:\n",
    "- **Input**: 128-D VGGish feature vector(s) per audio mix\n",
    "- **Output**: One-hot encoded label of out-of-tune instrument\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0743484",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "11cbb301",
   "metadata": {},
   "source": [
    "## Methodology <a name=\"methodology\"></a>\n",
    "\n",
    "### Step-by-Step:\n",
    "1. **Data Augmentation**:\n",
    "   - Generate pitch-shifted multi-instrument samples\n",
    "   - Create metadata file (`labels.csv`)\n",
    "2. **Feature Extraction**:\n",
    "   - Extract 128-D embeddings using VGGish\n",
    "3. **Label Encoding**:\n",
    "   - One-hot encode instruments and tuning status\n",
    "4. **Train/Test Split**:\n",
    "   - Standard 80/20 split or stratified by instrument\n",
    "5. **Classifier Training**:\n",
    "   - Fit baseline classifier\n",
    "6. **Evaluation**:\n",
    "   - Generate metrics, confusion matrix, and visualizations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1218ac",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c185c4f0",
   "metadata": {},
   "source": [
    "## Preliminary Analysis & Results <a name=\"results\"></a>\n",
    "\n",
    "We trained the initial model on a subset of 100 samples. Below are key findings:\n",
    "\n",
    "### Results (Sample):\n",
    "- Accuracy: XX%\n",
    "- F1-score (macro): XX%\n",
    "- Instruments like [X] show higher confusion with [Y]\n",
    "\n",
    "### Visualization:\n",
    "> *(Insert matplotlib/seaborn Confusion Matrix or PCA projection here)*\n",
    "\n",
    "### Interpretation:\n",
    "- Certain instrument combinations may mask pitch shifts.\n",
    "- VGGish may be sensitive to harmonics rather than pitch center in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c2d49c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b604e1c",
   "metadata": {},
   "source": [
    "## Evaluation Metrics <a name=\"evaluation\"></a>\n",
    "\n",
    "To evaluate the classifier's ability to identify the out-of-tune instrument:\n",
    "\n",
    "- **Accuracy**\n",
    "- **Confusion Matrix**\n",
    "- **Precision / Recall / F1-score** (macro and per-class)\n",
    "- **Top-1 and Top-2 accuracy**\n",
    "- **AUC-ROC** (optional for binary tuning detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a248a982",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e9ca777",
   "metadata": {},
   "source": [
    "## Ethical Considerations <a name=\"ethics\"></a>\n",
    "\n",
    "- Model trained on synthetic data — may not generalize to real-world performances\n",
    "- Risk of overfitting to artifacts introduced during synthetic pitch shifting\n",
    "- Use of tuning detection in artistic expression must respect creative freedom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4647fce5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "080f45f7",
   "metadata": {},
   "source": [
    "## Future Work <a name=\"future\"></a>\n",
    "\n",
    "- Integrate source separation (e.g., Demucs, Spleeter) for per-stream analysis\n",
    "- Test on real-world recordings from MusicNet or user-generated audio\n",
    "- Build web or real-time tool for live instrument tuning analysis\n",
    "- Train contrastive learning model using positive (in-tune) vs. negative (out-of-tune) pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bd1f34",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d9982c",
   "metadata": {},
   "source": [
    "## References <a name=\"references\"></a>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
